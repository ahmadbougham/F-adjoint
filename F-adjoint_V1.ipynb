{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# **F-adjoint concept**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pc8wedrIthWk"
      },
      "id": "pc8wedrIthWk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We consider the simple case of a fully-connected deep multi-layer perceptron (MLP) composed of $L$ layers trained in a supervised setting.  We will denote such an architecture by $ A[N_0, \\cdots, N_\\ell,\\cdots, N_L]$ where $N_0$ is the size of the input layer, $N_\\ell$ is the size of hidden layer $\\ell$,\n",
        "and $N_L$ is the size of the output layer; $L$ is defined as the depth of the  (MLP).\n",
        "\n",
        "**Notations**\n",
        "\n",
        "$$\n",
        "  \\begin{array}{|l|l|}\n",
        "  \\hline\\\\\n",
        "  Term & Description \\\\ \\hline\n",
        " W^{\\ell}\\in\\mathbb{R}^{N_{\\ell}\\times N_{\\ell-1}} & \\mathrm{Weight\\ matrix\\ of\\ the\\ layer}\\ \\ell  \\\\ \\hline\n",
        " Y^{\\ell}\\in\\mathbb{R}^{N_{\\ell}}  & \\mathrm{Preactivation\\ vector\\ at\\ layer}\\ {\\ell}, Y^{\\ell} = W^{\\ell}X^{\\ell-1} \\\\ \\hline\n",
        " X^{\\ell}\\in\\mathbb{R}^{N_{\\ell}} & \\mathrm{Activition\\ vector\\ at\\ the\\ layer} \\ {\\ell}, X^{\\ell} =\\sigma^{\\ell}(Y^{\\ell})\\\\ \\hline\n",
        " \\sigma^\\ell:\\  \\mathbb{R}^{N_{\\ell}}\\ni Y^{\\ell}\\longmapsto\\sigma^{\\ell}(Y^{\\ell})\\in\\mathbb{R}^{N_{\\ell}} & \\mathrm{Coordinate-wise\\ activition\\ function\\ of\\ the\\ layer}\\ {\\ell}\\\\ \\hline\n",
        "   \\end{array}$$\n",
        ""
      ],
      "metadata": {
        "id": "9FQfr3Qygrlk"
      },
      "id": "9FQfr3Qygrlk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The F-propagation and F-adjoint\n",
        "\n",
        "We   shall recall and revise the  definition of the this notion and provide some straightforward properties and improvements of this adjoint-like representation.\n",
        "\n",
        "\n",
        "\n",
        "## Definition of an $F$-propagation\n",
        "\n",
        "Let $X^0\\in\\mathbb{R}^{N_0}$ be a given data, $\\sigma$ be a coordinate-wise map from $\\mathbb{R}^{N_\\ell}$ into $\\mathbb{R}^{N_{\\ell}}$ and $W^{\\ell}\\in \\mathbb{R}^{{N_{\\ell}}\\times{N_{\\ell-1}}}$ for all ${1\\leq \\ell\\leq L}$. We say that we have a two-step recursive F-propagation   $F$  through the (MLP) $A[N_0,\\cdots, N_L]$ if   one has the following family of vectors\n",
        "$$\n",
        "F(X^0):=\\begin{Bmatrix}X^{0},Y^{1},X^{1},\\cdots,X^{L-1},Y^{L},X^{L}\\end{Bmatrix}\\  \\mathrm{where}\\  Y^\\ell=W^{\\ell}X^{\\ell-1}, \\ X^\\ell=\\sigma(Y^\\ell),\\ \\ell=1,\\cdots, L.\n",
        "$$\n",
        "Before going further, let us point that in the above definition the prefix \"F\" stands for \"Feed-forward\".\n",
        "\n",
        "## Definition of the $F$-adjoint of an $F$-propagation\n",
        "\n",
        "Let $X^0\\in\\mathbb{R}^{N_0}$ be a given data and let  $X^L_*\\in\\mathbb{R}^{N_L}$ be a given vector.  We define the F-adjoint propagation  ${F}_{*}$, through the (MLP) $A[N_0,\\cdots, N_L]$, associated to the F-propagation  $F(X^0)$  as follows\n",
        "$$\n",
        "F_{*}(X^{0}, X^{L}_{*}):=\\begin{Bmatrix}X^{L}_{*}, Y^{L}_{*}, X^{L-1}_{*},\\cdots, X^{1}_{*},Y^{1}_{*}, X^{0}_{*} \\end{Bmatrix}\\  \\mathrm{where}\\  Y^\\ell_{*}=X^{\\ell}_{*}\\odot {\\sigma}'(Y^\\ell), \\ X^{\\ell-1}_{*}=(W^\\ell)^\\top Y^\\ell_{*},\\ \\ell=L,\\cdots, 1.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Reference\n",
        "\n",
        "<div id=\"refs\" class=\"references\">\n",
        "\n",
        "\n",
        "Boughammoura, A. (2023). Backpropagation and F-adjoint. arXiv preprint arXiv:2304.13820.(https://arxiv.org/abs/2304.13820)\n",
        "\n",
        "</div>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xt_PQbXWljC4"
      },
      "id": "Xt_PQbXWljC4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a consequense, one may rewrite the  algorithm as follows:\n",
        "\n",
        "1. Require: $x,y$\n",
        "2. Ensure: $W:=(W^\\ell)_{1\\leq \\ell\\leq L}$\n",
        "\n",
        "   1.  Initialize $W^\\ell , \\ell=1,\\ldots,L$\n",
        "   2.  Function: $F$-propagation($x,W,\\sigma$)\n",
        "    \n",
        "        1.  $X^0\\leftarrow x$\n",
        "        2.  $F\\leftarrow\\{X^0\\}$\n",
        "        3.  For $\\ell=1$ to $L$:\n",
        "                            \n",
        "            $Y^\\ell\\leftarrow W^\\ell X^{\\ell-1}$\n",
        "                 \n",
        "            $X^\\ell\\leftarrow\\sigma(Y^{\\ell})$\n",
        "            \n",
        "            $F\\leftarrow Y^\\ell,X^\\ell$\n",
        "            \n",
        "            End For\n",
        "          \n",
        "       Return $F$\n",
        "   "
      ],
      "metadata": {
        "id": "WBfNv3NppYzO"
      },
      "id": "WBfNv3NppYzO"
    },
    {
      "cell_type": "markdown",
      "source": [
        " Function: $F_*$-propagation($F, J, y, \\sigma',\\eta$)\n",
        "\n",
        "\n",
        "     $$X_*^L\\leftarrow \\frac{\\partial J}{\\partial X^L}(X^L, y)$$\n",
        "\n",
        "      2. For $\\ell= L$ to $1$:                              \n",
        "         $Y_*^\\ell \\leftarrow X_*^{\\ell}\\odot\\sigma'(Y^\\ell) $\n",
        "          \n",
        "           $X^{\\ell-1}_{*} \\leftarrow ({W^{\\ell}})^\\top Y^\\ell_{*}$\n",
        "                                \n",
        "           $F_*\\leftarrow Y_*^\\ell, X^{\\ell-1}_{*}$\n",
        "           \n",
        "           End For\n",
        "           \n",
        "       Return $F_*$\n",
        ""
      ],
      "metadata": {
        "id": "IOoFXAHNpzq1"
      },
      "id": "IOoFXAHNpzq1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2381474a",
      "metadata": {
        "id": "2381474a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# The activation function and it's derivative\n",
        "sigma = lambda x: 1/(1+np.exp(-x))\n",
        "d_sigma= lambda x: sigma(x)*(1-sigma(x))\n",
        "\n",
        "# The F-propagation function\n",
        "def F_prop(X, W):\n",
        "    L=len(W)\n",
        "    X=X=np.append(X, np.ones((X.shape[0],1)), axis = 1)\n",
        "    F_X = {'X0':X.T}\n",
        "    for h in range(1, L+1):\n",
        "        F_X[f'Y{h}']=np.dot(W[f'W{h}'], F_X[f'X{h-1}'])\n",
        "        if h!=L:\n",
        "            F_X[f'X{h}']=np.concatenate((sigma(F_X[f'Y{h}']), np.ones((1,X.shape[0]))), axis = 0)\n",
        "        else:\n",
        "            F_X[f'X{h}']=sigma(F_X[f'Y{h}'])\n",
        "    return F_X\n",
        "\n",
        "# The F-adjoint propagation function\n",
        "def F_star_prop(X,y, W):\n",
        "    L=len(W)\n",
        "    F_X = F_prop(X, W)\n",
        "    FX_star={f'X*{L}':(F_X[f'X{L}']-y)}\n",
        "    Y_star={}\n",
        "    for h in reversed(range(1, L+1)):\n",
        "        FX_star[f'Y*{h}']=FX_star[f'X*{h}']*(d_sigma(F_X[f'Y{h}']))\n",
        "        FX_star[f'X*{h-1}']=np.delete(W[f'W{h}'], -1, axis = 1).T.dot(FX_star[f'Y*{h}'])\n",
        "    return FX_star\n",
        "\n",
        "# Update the weights layer-wise by using the F-adjoint propagation: local-learnig approach\n",
        "def F_star_W(X,y, W,eta=0.5):\n",
        "    L=len(W)\n",
        "    F_X = F_prop(X, W)\n",
        "    FX_star={'X*2':(F_X['X2']-y)}\n",
        "    Y_star={}\n",
        "    ##\n",
        "    FX_star['Y*2']=FX_star['X*2']*(d_sigma(F_X['Y2']))\n",
        "    W['W2']= W['W2'] -eta*FX_star['Y*2'].dot(F_X['X1'].T)\n",
        "    ##\n",
        "    FX_star['X*1']=np.delete(W['W2'], -1, axis = 1).T.dot(FX_star['Y*2'])\n",
        "    FX_star['Y*1']=FX_star['X*1']*(d_sigma(F_X['Y1']))\n",
        "    W['W1']= W['W1'] -eta*FX_star['Y*1'].dot(F_X['X0'].T)\n",
        "    return W\n",
        "\n",
        "# Update the weights globally by using the F-adjoint propagation: nonlocal-learnig approach\n",
        "def Grad_star(X,y, W, eta=0.5):\n",
        "    L=len(W)\n",
        "    F_X = F_prop(X, W)\n",
        "    FX_star=F_star_prop(X,y, W)\n",
        "    Grad={}\n",
        "    for h in range(1, L+1):\n",
        "        Grad[f'W{h}']=W[f'W{h}']-eta*FX_star[f'Y*{h}'].dot(F_X[f'X{h-1}'].T)\n",
        "    return Grad"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F-adjoint propagation and application to local and nonlocal learning"
      ],
      "metadata": {
        "id": "GLicAEMpmI9u"
      },
      "id": "GLicAEMpmI9u"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute the $F$ and $F_*$ propagations of the XOR data**"
      ],
      "metadata": {
        "id": "ARakAdm6uALH"
      },
      "id": "ARakAdm6uALH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c3588e3",
      "metadata": {
        "id": "4c3588e3"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=3)\n",
        "# Learning the XOR dataset\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 1, 1, 0])\n",
        "#X = np.array([[0, 0]]); y=np.array([[1]])\n",
        "W1=np.array([[1, -1, 1], [-1, 1, -1]])\n",
        "W2=np.array([[1, -1, 1]])\n",
        "W={'W1':W1, 'W2':W2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ff6e1ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ff6e1ba",
        "outputId": "7a337af9-536d-4c19-b7dd-0d0731e86c53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'X0': array([[0., 0., 1., 1.],\n",
              "        [0., 1., 0., 1.],\n",
              "        [1., 1., 1., 1.]]),\n",
              " 'Y1': array([[ 1.,  0.,  2.,  1.],\n",
              "        [-1.,  0., -2., -1.]]),\n",
              " 'X1': array([[0.731, 0.5  , 0.881, 0.731],\n",
              "        [0.269, 0.5  , 0.119, 0.269],\n",
              "        [1.   , 1.   , 1.   , 1.   ]]),\n",
              " 'Y2': array([[1.462, 1.   , 1.762, 1.462]]),\n",
              " 'X2': array([[0.812, 0.731, 0.853, 0.812]])}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "F_prop(X, W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30c77d09",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30c77d09",
        "outputId": "c044f502-923f-46a4-fd79-76688b219d54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'X*2': array([[ 0.812, -0.269, -0.147,  0.812]]),\n",
              " 'Y*2': array([[ 0.124, -0.053, -0.018,  0.124]]),\n",
              " 'X*1': array([[ 0.124, -0.053, -0.018,  0.124],\n",
              "        [-0.124,  0.053,  0.018, -0.124]]),\n",
              " 'Y*1': array([[ 0.024, -0.013, -0.002,  0.024],\n",
              "        [-0.024,  0.013,  0.002, -0.024]]),\n",
              " 'X*0': array([[ 0.049, -0.026, -0.004,  0.049],\n",
              "        [-0.049,  0.026,  0.004, -0.049]])}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "F_star_prop(X,y, W)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning the XOR dataset with layer-wise function: F_star_W(X,y, W)**"
      ],
      "metadata": {
        "id": "CaEj0gkbt48T"
      },
      "id": "CaEj0gkbt48T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "531ca8f5",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "531ca8f5",
        "outputId": "f4a6caa7-eb53-402f-dd9a-68576356990a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X2 [[0.812 0.731 0.853 0.812]] 1\n",
            "================================\n",
            "X2 [[0.781 0.707 0.833 0.78 ]] 2\n",
            "================================\n",
            "X2 [[0.746 0.681 0.81  0.743]] 3\n",
            "================================\n",
            "X2 [[0.706 0.654 0.784 0.7  ]] 4\n",
            "================================\n",
            "X2 [[0.662 0.627 0.758 0.654]] 5\n",
            "================================\n",
            "X2 [[0.618 0.602 0.733 0.607]] 6\n",
            "================================\n",
            "X2 [[0.575 0.582 0.712 0.561]] 7\n",
            "================================\n",
            "X2 [[0.535 0.567 0.695 0.518]] 8\n",
            "================================\n",
            "X2 [[0.499 0.558 0.684 0.48 ]] 9\n",
            "================================\n"
          ]
        }
      ],
      "source": [
        "# Learning the XOR dataset with layer-wise function: F_star_W(X,y, W)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 1, 1, 0])\n",
        "#X = np.array([[0, 0]]); y=np.array([[1]])\n",
        "W1=np.array([[1, -1, 1], [-1, 1, -1]])\n",
        "W2=np.array([[1, -1, 1]])\n",
        "W={'W1':W1, 'W2':W2}\n",
        "##\n",
        "X2=F_prop(X, W)['X2']\n",
        "##\n",
        "X2_c=np.where(X2>=0.5,1,0)\n",
        "k=0\n",
        "while (X2_c -y).any() != 0 :\n",
        "#for k in range(100):\n",
        "    #print('X',X)\n",
        "    X2=F_prop(X, W)['X2']\n",
        "    X2_c=np.where(X2>=0.5,1,0)\n",
        "    FX_star=F_star_prop(X,y, W)\n",
        "\n",
        "\n",
        "    print('X2',X2,k+1)\n",
        "    W=F_star_W(X,y, W)\n",
        "    #W=Grad_star(X,y, W)\n",
        "    X=X-FX_star['X*0'].T\n",
        "    k+=1\n",
        "    print(\"================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us notes that the number of itairations for this learning task is 9.\n"
      ],
      "metadata": {
        "id": "XYXisIJQtD_x"
      },
      "id": "XYXisIJQtD_x"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning the XOR dataset with global gradient function:Grad_star(X,y, W)**"
      ],
      "metadata": {
        "id": "v_W7R_AJuUIx"
      },
      "id": "v_W7R_AJuUIx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afad418e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afad418e",
        "outputId": "04609b95-cd80-4308-e14d-672ae7c278fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X2= [[0.818 0.622 0.881 0.818]] 1\n",
            "================================\n",
            "X2= [[0.481 0.713 0.862 0.481]] 2\n",
            "================================\n"
          ]
        }
      ],
      "source": [
        "# Learning the XOR dataset with global gradient function:Grad_star(X,y, W)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 1, 1, 0])\n",
        "#X = np.array([[0, 0]]); y=np.array([[1]])*np.sqrt(6)\n",
        "W1=np.array([[1, -1, 1], [-1, 1, 0]])*(20)\n",
        "W2=np.array([[1, -1, 1]])\n",
        "W={'W1':W1, 'W2':W2}\n",
        "##\n",
        "X2=F_prop(X, W)['X2']\n",
        "##\n",
        "X2_c=np.where(X2>=0.5,1,0)\n",
        "k=0\n",
        "while (X2_c -y).any() != 0 :\n",
        "#for k in range(10):\n",
        "    #print('X',X)\n",
        "    X2=F_prop(X, W)['X2']\n",
        "    X2_c=np.where(X2>=0.5,1,0)\n",
        "    FX_star=F_star_prop(X,y, W)\n",
        "\n",
        "\n",
        "    print('X2=',X2,k+1)\n",
        "    #W=F_star_W(X,y, W)\n",
        "    W=Grad_star(X,y, W)\n",
        "    X=X-FX_star['X*0'].T##(X0-Xstar0) as input\n",
        "    k+=1\n",
        "    print(\"================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us notes that the number of itairations for this learning task is 2."
      ],
      "metadata": {
        "id": "JgVilgZDtdjX"
      },
      "id": "JgVilgZDtdjX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8f52aff",
      "metadata": {
        "id": "a8f52aff"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}