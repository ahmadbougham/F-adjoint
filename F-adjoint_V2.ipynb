{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# The activation function and it's derivative\n",
    "sigma = lambda x: 1/(1+np.exp(-x))\n",
    "d_sigma= lambda x: sigma(x)*(1-sigma(x))\n",
    "\n",
    "# The F-propagation function\n",
    "def F_prop(X, W):\n",
    "    L=len(W)\n",
    "    X=X=np.append(X, np.ones((X.shape[0],1)), axis = 1)\n",
    "    F_X = {'X0':X.T}\n",
    "    for h in range(1, L+1):\n",
    "        F_X[f'Y{h}']=np.dot(W[f'W{h}'], F_X[f'X{h-1}'])\n",
    "        if h!=L:\n",
    "            F_X[f'X{h}']=np.concatenate((sigma(F_X[f'Y{h}']), np.ones((1,X.shape[0]))), axis = 0)\n",
    "        else:\n",
    "            F_X[f'X{h}']=sigma(F_X[f'Y{h}'])\n",
    "    return F_X\n",
    "\n",
    "# The F-adjoint propagation function\n",
    "def Fstar_prop(X,y, W):\n",
    "    L=len(W)\n",
    "    F_X = F_prop(X, W)\n",
    "    FX_star={f'X*{L}':(F_X[f'X{L}']-y)}\n",
    "    Y_star={}\n",
    "    for h in reversed(range(1, L+1)):\n",
    "        FX_star[f'Y*{h}']=FX_star[f'X*{h}']*(d_sigma(F_X[f'Y{h}']))\n",
    "        FX_star[f'X*{h-1}']=np.delete(W[f'W{h}'], -1, axis = 1).T.dot(FX_star[f'Y*{h}'])\n",
    "    return FX_star\n",
    "\n",
    "# Update the weights layer-wise by using the F-adjoint propagation: local-learnig approach\n",
    "def Fstar_W(X,y, W,eta1=0.5,eta2=0.5):\n",
    "    L=len(W)\n",
    "    F_X = F_prop(X, W)\n",
    "    FX_star={'X*2':(F_X['X2']-y)}\n",
    "    Y_star={}\n",
    "    ## Update of the layer 2 with learning rate eta2\n",
    "    FX_star['Y*2']=FX_star['X*2']*(d_sigma(F_X['Y2']))\n",
    "    W['W2']= W['W2'] -eta2*FX_star['Y*2'].dot(F_X['X1'].T)\n",
    "    ## Update of the layer 1 with learning rate eta1\n",
    "    FX_star['X*1']=np.delete(W['W2'], -1, axis = 1).T.dot(FX_star['Y*2'])\n",
    "    FX_star['Y*1']=FX_star['X*1']*(d_sigma(F_X['Y1']))\n",
    "    W['W1']= W['W1'] -eta1*FX_star['Y*1'].dot(F_X['X0'].T)\n",
    "    return W\n",
    "\n",
    "# Update the weights globally by using the F-adjoint propagation: nonlocal-learnig approach\n",
    "def Grad_star(X,y, W, eta=0.5):\n",
    "    L=len(W)\n",
    "    F_X = F_prop(X, W)\n",
    "    FX_star=Fstar_prop(X,y, W)\n",
    "    Grad={}\n",
    "    for h in range(1, L+1):\n",
    "        Grad[f'W{h}']=W[f'W{h}']-eta*FX_star[f'Y*{h}'].dot(F_X[f'X{h-1}'].T)\n",
    "    return Grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
